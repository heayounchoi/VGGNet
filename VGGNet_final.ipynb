{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epr-gytcc7B8",
        "outputId": "166e1ae7-364d-4f4f-f9e0-9a38a8c2aced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "checkpoints = '/content/drive/MyDrive/Colab Notebooks/VGGNet/checkpoints/'\n",
        "if not os.path.exists(checkpoints):\n",
        "    os.makedirs(checkpoints)"
      ],
      "metadata": {
        "id": "grWWJ68tdrnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5KfP_j8el47n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "fixed_config = {\n",
        "    \"scheduler\": \"CosineAnnealingLR\",\n",
        "    \"weight_decay\": 5E-04,\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"epochs\": 200,\n",
        "    \"transform_normalize\": ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    \"classifier\": \"avg\",\n",
        "    \"batch_size\": 128,\n",
        "    \"dropout_rate\": 0.5,\n",
        "}\n",
        "\n",
        "sweep_config = {\n",
        "    \"method\": \"grid\",\n",
        "    \"metric\": {\"goal\": \"minimize\", \"name\": \"loss\"},\n",
        "    \"parameters\": {\n",
        "        \"nesterov\": {\"values\": [False]}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "8REIgQmOkNzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, 4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(fixed_config[\"transform_normalize\"][0], fixed_config[\"transform_normalize\"][1])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(fixed_config[\"transform_normalize\"][0], fixed_config[\"transform_normalize\"][1])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"/content/drive/MyDrive/Colab Notebooks/VGGNet/dataset/\",\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=transform)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root=\"/content/drive/MyDrive/Colab Notebooks/VGGNet/dataset/\",\n",
        "                                train=False,\n",
        "                                download=True,\n",
        "                                transform=transform_test)\n",
        "\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "validation_size = len(train_dataset) - train_size\n",
        "\n",
        "train_dataset, validation_dataset = random_split(train_dataset, [train_size, validation_size])"
      ],
      "metadata": {
        "id": "ahtvUXsNmWRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = fixed_config[\"batch_size\"]\n",
        "dropout_rate = fixed_config[\"dropout_rate\"]\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True)\n",
        "\n",
        "validation_loader = DataLoader(dataset=validation_dataset,\n",
        "                               batch_size=BATCH_SIZE,\n",
        "                               shuffle=False)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=BATCH_SIZE,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "91mwq3mEExu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_size = 0\n",
        "for (X_train, Y_train) in train_loader:\n",
        "    X_train_size = X_train.size()[1:]\n",
        "    print(X_train_size)\n",
        "    print(f\"X_train: {X_train.size()} type: {X_train.type()}\")\n",
        "    print(f\"Y_train: {Y_train.size()} type: {Y_train.type()}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO1599Hcngo6",
        "outputId": "79ed558f-804d-446c-a891-e2002572fe0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 32, 32])\n",
            "X_train: torch.Size([128, 3, 32, 32]) type: torch.FloatTensor\n",
            "Y_train: torch.Size([128]) type: torch.LongTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(\"사용하는 Device :\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfMM7Gopny0m",
        "outputId": "309243ef-f151-47f0-a297-77e6e88d7700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용하는 Device : cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGNet(nn.Module):\n",
        "    def __init__(self, features, fc_units=512):\n",
        "        super(VGGNet, self).__init__()\n",
        "        self.features = features\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self._feature_map_size = self._get_feature_map_size(features)\n",
        "\n",
        "        self.fc = self._get_classifier(fc_units)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu' )\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal(m.weight, std=1e-3)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def _get_feature_map_size(self, features):\n",
        "        x = torch.randn(1, *X_train_size)\n",
        "        x = features(x)\n",
        "        return torch.flatten(x, 1).size(1)\n",
        "\n",
        "    def _get_classifier(self, fc_units):\n",
        "        if fixed_config[\"classifier\"] == \"fc\":\n",
        "            classifier = nn.Sequential(\n",
        "                nn.Linear(self._feature_map_size, fc_units),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=dropout_rate),\n",
        "                nn.Linear(fc_units, fc_units),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=dropout_rate),\n",
        "                nn.Linear(fc_units, num_classes)\n",
        "            )\n",
        "        elif fixed_config[\"classifier\"] == \"avg\":\n",
        "            classifier = nn.Sequential(\n",
        "                nn.Linear(self._feature_map_size, num_classes)\n",
        "            )\n",
        "        return classifier\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def make_layers(cfg):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'maxpool':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            if v=='conv1':\n",
        "                v = in_channels\n",
        "                conv2d = nn.Conv2d(in_channels, v, kernel_size=1)\n",
        "            else:\n",
        "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            in_channels = v\n",
        "            layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "    if fixed_config[\"classifier\"] == \"avg\":\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "min_width = 64\n",
        "\n",
        "cfgs = {\n",
        "    'A': [min_width, 'maxpool',\n",
        "          min_width*2, 'maxpool',\n",
        "          min_width*4, min_width*4, 'maxpool',\n",
        "          min_width*8, min_width*8, 'maxpool',\n",
        "          min_width*8, min_width*8, 'maxpool'],\n",
        "    'B': [min_width, min_width, 'maxpool',\n",
        "          min_width*2, min_width*2, 'maxpool',\n",
        "          min_width*4, min_width*4, 'maxpool',\n",
        "          min_width*8, min_width*8, 'maxpool',\n",
        "          min_width*8, min_width*8, 'maxpool'],\n",
        "    'C': [min_width, min_width, 'maxpool',\n",
        "          min_width*2, min_width*2, 'maxpool',\n",
        "          min_width*4, min_width*4, 'conv1', 'maxpool',\n",
        "          min_width*8, min_width*8, 'conv1', 'maxpool',\n",
        "          min_width*8, min_width*8, 'conv1', 'maxpool'],\n",
        "    'D': [min_width, min_width, 'maxpool',\n",
        "          min_width*2, min_width*2, 'maxpool',\n",
        "          min_width*4, min_width*4, min_width*4, 'maxpool',\n",
        "          min_width*8, min_width*8, min_width*8, 'maxpool',\n",
        "          min_width*8, min_width*8, min_width*8, 'maxpool'],\n",
        "    'E': [min_width, min_width, 'maxpool',\n",
        "          min_width*2, min_width*2, 'maxpool',\n",
        "          min_width*4, min_width*4, min_width*4, min_width*4, 'maxpool',\n",
        "          min_width*8, min_width*8, min_width*8, min_width*8, 'maxpool',\n",
        "          min_width*8, min_width*8, min_width*8, min_width*8, 'maxpool']\n",
        "}\n",
        "\n",
        "def vgg11(): # configuration A\n",
        "    return VGGNet(make_layers(cfgs['A']))\n",
        "\n",
        "def vgg13(): # configuration B\n",
        "    return VGGNet(make_layers(cfgs['B']))\n",
        "\n",
        "def vgg16_1(): # configuration C\n",
        "    return VGGNet(make_layers(cfgs['C']))\n",
        "\n",
        "def vgg16(): # configuration D\n",
        "    return VGGNet(make_layers(cfgs['D']))\n",
        "\n",
        "def vgg19(): # configuration E\n",
        "    return VGGNet(make_layers(cfgs['E']))"
      ],
      "metadata": {
        "id": "2dOlRTQxn0Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = vgg16().to(DEVICE)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(model)\n",
        "print(f\"총 파라미터 개수: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_epxNHflHacz",
        "outputId": "8870d927-cc51-43ec-a09d-c7276a9bd2e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-c8cd71f2bbd6>:20: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  nn.init.normal(m.weight, std=1e-3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGGNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "총 파라미터 개수: 14728266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def train(config):\n",
        "    criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
        "    total_epochs = fixed_config[\"epochs\"]\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoints + 'VGGNet_ablation_nesterov/epoch93')\n",
        "        old_model_state_dict = checkpoint['model_state_dict']\n",
        "        new_model_state_dict = model.state_dict()\n",
        "\n",
        "        for name, param in old_model_state_dict.items():\n",
        "            if name in new_model_state_dict:\n",
        "                try:\n",
        "                    new_model_state_dict[name].copy_(param)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to copy param: {name}, due to {e}\")\n",
        "\n",
        "        model.load_state_dict(new_model_state_dict, strict=False)\n",
        "\n",
        "        last_epoch = checkpoint['epoch']\n",
        "        best_val_loss = checkpoint[\"best_val_loss\"]\n",
        "\n",
        "    except:\n",
        "        checkpoint = None\n",
        "        last_epoch = -1\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "    finally:\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=fixed_config[\"learning_rate\"], momentum=0.9,\n",
        "                                    weight_decay=fixed_config[\"weight_decay\"], nesterov=config.nesterov)\n",
        "        if fixed_config[\"scheduler\"] == \"CosineAnnealingLR\":\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epochs)\n",
        "        elif fixed_config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
        "            scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "        if checkpoint and 'optimizer_state_dict' in checkpoint:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in tqdm(range(last_epoch + 1, total_epochs), desc='Epoch Progress'):\n",
        "            avg_cost = 0\n",
        "\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            correct_train = 0\n",
        "            total_train = 0\n",
        "\n",
        "            with tqdm(total=len(train_loader), desc='Batch Progress') as batch_bar:\n",
        "                for X, Y in train_loader:\n",
        "                    X = X.to(DEVICE)\n",
        "                    Y = Y.to(DEVICE)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    hypothesis = model(X)\n",
        "                    loss = criterion(hypothesis, Y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    train_loss += loss.item()\n",
        "\n",
        "                    _, predicted_train = torch.max(hypothesis.data, 1)\n",
        "                    total_train += Y.size(0)\n",
        "                    correct_train += (predicted_train == Y).sum().item()\n",
        "\n",
        "                    batch_bar.update()\n",
        "\n",
        "                train_losses.append(train_loss / len(train_loader))\n",
        "                train_accuracy = (100 * correct_train) / total_train\n",
        "\n",
        "                model.eval()\n",
        "                val_loss = 0.0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    correct = 0\n",
        "                    total = 0\n",
        "                    for X, Y in validation_loader:\n",
        "                        X = X.to(DEVICE)\n",
        "                        Y = Y.to(DEVICE)\n",
        "\n",
        "                        output = model(X)\n",
        "                        _, predicted = torch.max(output, 1)\n",
        "\n",
        "                        val_loss += criterion(output, Y).item()\n",
        "\n",
        "                        total += Y.size(0)\n",
        "                        correct += (predicted == Y).sum().item()\n",
        "\n",
        "                    val_losses.append(val_loss / len(validation_loader))\n",
        "                    val_accuracy = correct / total\n",
        "                    scheduler.step()\n",
        "\n",
        "                train_desc = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'train_accuracy': train_accuracy,\n",
        "                'val_accuracy': val_accuracy * 100,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'best_val_loss': best_val_loss\n",
        "                }\n",
        "\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    torch.save(train_desc, checkpoints + f'VGGNet_ablation_nesterov/best_epoch')\n",
        "\n",
        "                torch.save(train_desc, checkpoints+f'VGGNet_ablation_nesterov/last_epoch')\n",
        "\n",
        "                wandb.log({\"train_accuracy\": train_accuracy, \"val_accuracy\": val_accuracy*100,\n",
        "                           \"train_losses\": train_losses[-1], \"val_losses\": val_losses[-1],\n",
        "                           \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "                           }, step=epoch)\n",
        "\n",
        "                print('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.4f}%, Val Loss: {:.4f}, Val Accuracy: {:.2f}%'\n",
        "                        .format(epoch, total_epochs, train_losses[-1], train_accuracy, val_losses[-1], val_accuracy*100))\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, Y in test_loader:\n",
        "                X = X.to(DEVICE)\n",
        "                Y = Y.to(DEVICE)\n",
        "\n",
        "                output = model(X)\n",
        "                loss = criterion(output, Y)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(output, 1)\n",
        "                total += Y.size(0)\n",
        "                correct += (predicted == Y).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "        test_accuracy = correct / total\n",
        "\n",
        "        wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n",
        "\n",
        "        wandb.alert(\"[nesterov]Training Task Finished\", f\"nesterov: {str(config.nesterov)}\")\n",
        "        return val_losses[-1]"
      ],
      "metadata": {
        "id": "1CHfsIH6uLoz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}